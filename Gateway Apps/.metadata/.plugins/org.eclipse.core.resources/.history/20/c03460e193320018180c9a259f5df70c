package org.nandor.spark;

import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.Timer;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;
import org.json.simple.parser.ParseException;

import scala.collection.mutable.HashMap;

public class Testing {

	public static JSONObject readJson(String file) {
		JSONParser parser = new JSONParser();
		JSONObject a = new JSONObject();
		try {
			a = (JSONObject) parser.parse(new FileReader(file));
		} catch (IOException | ParseException e) {
			e.printStackTrace();
		}
		return a;
	}

	public static void writeJson(String file, JSONObject json) {
		FileWriter f;
		try {
			f = new FileWriter(file);
			f.write(json.toJSONString());
			f.flush();
			f.close();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	public static void main(String[] args) {
		// Timer

		Timer timer = new Timer();
		// timer.schedule(Methods.timerTask, 300, 600000);

		Methods.GAGlobalStuff(f, 60, 1000,true);
	    if (f.checkIfAppsAllocated().size()==0){
	    	writeHDFS(sc,fogFile,"GlobalGA",Exporter.writeStringFog(f)); 
	    }
		
		
	     if (Methods.AdvClustering(f, (float)1.2, 8)) {
				// Methods.ResourceAllocation(f);
				Methods.nandorsAlphaResourceAlloc(f);
				Methods.displayClsAndRes(f);
				System.out.println(Methods.GAClusStuff(f, 40, 1000, true));
			}else{
				System.out.println("Clustering New  with "+1.2+" and "+8+" failed!");
			}
	    f.deployClusters();
	    if (f.checkIfAppsAllocated().size()==0){
		    writeHDFS(sc,fogFile,"ClustAdvGA",Exporter.writeStringFog(f));
	    }
	    
	    if (Methods.Clustering(f, 1, 8)) {
			// Methods.ResourceAllocation(f);
			Methods.nandorsAlphaResourceAlloc(f);
			Methods.displayClsAndRes(f);
			System.out.println(Methods.GAClusStuff(f, 40, 1000, true));
		}else{
			System.out.println("Clustering Old with "+1+" and "+8+" failed!");
		}
	    f.deployClusters();
	    if (f.checkIfAppsAllocated().size()==0){
		    writeHDFS(sc,fogFile,"ClustOldGA",Exporter.writeStringFog(f));
	    }
	}

	public static void main2(String[] args) {

		// Init
		//Fog f = Methods.InitFog(1, 0);// Cluster Count, Cloud Gw Count
		//writeJson("C:/Users/Nandor/Documents/FogOfThings/Gateway Apps/spark-test/src/main/java/org/nandor/spark/deploy-W.json",Exporter.writeJsonFog(f));
		Fog f=Exporter.readJsonFog(readJson("C:/Users/Nandor/Documents/FogOfThings/Gateway Apps/spark-test/src/main/java/org/nandor/spark/deploy-W.json"));
		//Methods.Clustering(f, 1, 2);
		AdvancedCls cls = new AdvancedCls(f);
		Clustering clsOld = new Clustering(f);
		System.out.println(cls.getNeighbours(6,(float)1.3,0));
		System.out.println(clsOld.getNeighbours(6,(float)1.3,0));
		Set<Integer> test = new HashSet<Integer>();
		test.add(1);test.add(2);test.add(3);
		Set<Integer> test2 = new HashSet<Integer>();
		test2.add(4);test2.add(5);
		System.out.println(cls.distanceToCluster(6,test));
		System.out.println(cls.distanceToCluster(6,test2));

	}
}
