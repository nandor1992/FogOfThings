package org.nandor.spark;

import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Random;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;
import org.json.simple.parser.ParseException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;

public class Spark{

	public static void writeHDFS(JavaSparkContext sc, String loc,String title, String data){
		Configuration hadoopConfig = new Configuration();
		hadoopConfig.set("fs.defaultFS","hdfs://10.0.0.7:9000"); 
		FileSystem hdfs;
		try {
			hdfs = FileSystem.get(hadoopConfig);
			FileUtil.fullyDelete(hdfs,new Path("/"+loc+"/"+title));
			FileUtil.fullyDelete(hdfs,new Path("/"+loc+"/"+title+".json"));
			//Data Save
			List<String> dat = new ArrayList<String>();
			dat.add(data);
			JavaRDD<String> writer = sc.parallelize(dat);
			writer.collect();
			writer.repartition(1).saveAsTextFile("hdfs://10.0.0.7:9000/"+loc+"/"+title);
			//Data Merge
			FileUtil.copyMerge(hdfs, new Path("/"+loc+"/"+title), hdfs, new Path("/"+loc+"/"+title+".json"), false, hadoopConfig, null);
			FileUtil.fullyDelete(hdfs,new Path("/"+loc+"/"+title));
		} catch (IllegalArgumentException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		}

	public static void createFolder(JavaSparkContext sc, String loc){
		Configuration hadoopConfig = new Configuration();
		hadoopConfig.set("fs.defaultFS","hdfs://10.0.0.7:9000"); 
		FileSystem hdfs;
		try {
			hdfs = FileSystem.get(hadoopConfig);
			hdfs.mkdirs(new Path("/"+loc));
		} catch (IllegalArgumentException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

	public static String readHDFS(JavaSparkContext sc,String file){
		JavaRDD<String> dataRDD = sc.textFile("hdfs://10.0.0.7:9000/"+file+".json");
		String data = dataRDD.collect().toString();
		return data;	
	}
	
	public static void DisplayData(Fog f) {
		/*for (Integer i : f.getApps().keySet()) {
			System.out.println(f.getApps().get(i).getInfo());
		}
		System.out.println(f.getApps().get(1).getAppLoad((float) 1) + " "
				+ f.getApps().get(1).getProcDelay((float) 56.0, (float) 1.0));
		*/
		System.out.println("----Displaying Fog----");
		/*System.out.println(f.toString());
		System.out.println("Apps: " + f.getApps());
		System.out.println("Apps of  1: "+f.getApps().get(1).getApps().keySet());
		System.out.println("Apps of  2: "+f.getApps().get(2).getApps().keySet());
		System.out.println("Apps of  3: "+f.getApps().get(3).getApps().keySet());
		System.out.println("Apps of  4: "+f.getApps().get(4).getApps().keySet());
		System.out.println("Gw1 1: "+f.getGateways().get(1).getInfo());
		System.out.println("Resource: " + f.getResources().toString());
		System.out.println("Gateways: " + f.getGateways().toString());
		System.out.println("Clusters: " + f.getClusters().toString());
		for (Integer i : f.getClusters().keySet()) {
			System.out.println("Cluster "+i+" Load: "+f.getClusters().get(i).getClusterLoad());
		}*/
		for (Integer i : f.getGateways().keySet()) {
			System.out.println("Gateway " + f.getGateways().get(i).getInfo());
			for (Integer j: f.getGateways().get(i).getCluster().keySet()){
				System.out.println("Gw "+i+" Cluster "+j+" Share: "+f.getGateways().get(i).getClusterShare(j));
			}
		}
		
		print("Fog Utility: "+f.getFogCompoundUtility());
		print("Fog Delay: "+f.getFogCompoundDelay());
		print("Fog Reliability: "+f.getFogCompoundReliability());
	}
	
	public static Fog InitFog(int ClsCount, int cloudGw){
		//Initialization and Generation
		Fog f = new Fog("Main Fog");
		float[] lat = {(float)8.97,(float)30.897};
		float[] lat2 = {(float)37.37,(float)87.89};
		float[] lat3 = {(float)2.37,(float)6.89};
		f.generateNewFog(ClsCount,(float)40,(float)65,(float)0.1,(float)0.05,lat,cloudGw,lat2,lat3,(float)0.7,(float)0.15);
		//Analysis part, of distributing Gw's to clusters		
		return f;
	}

	public static float GAGlobalStuff(Fog f,int size,int cnt){
		print("----- GA Stuff -----");
		Genetic g = new Genetic(f);
		long start=System.currentTimeMillis();
		f.setDeplpyment(g.GAGlobal(size,cnt));
		f.deployFog();
		print("Fog Utility: "+f.getFogCompoundUtility());
		float tot_sec=(System.currentTimeMillis()-start)/(float)1000;
		System.out.println("Finished First Part in:"+tot_sec);
		System.out.println("Unalocated Apps: "+f.checkIfAppsAllocated());	
		return tot_sec;
	}
	
	public static float GAClusStuff(Fog f,int size,int cnt){
		print("----- GA Stuff -----");
		Genetic g = new Genetic(f);
		long start=System.currentTimeMillis();
		Clustering(f);
		ResourceAllocation(f);
		//DisplayData(f);
		for (Integer i: f.getClusters().keySet()){
			f.getClusters().get(i).setDeployment(g.GACluster(size,cnt, f.getClusters().get(i)));
		}
		f.deployClusters();
		float tot_sec=(System.currentTimeMillis()-start)/(float)1000;
		print("Fog Utility: "+f.getFogCompoundUtility());
		System.out.println("Finished Second Part in:"+tot_sec);
		System.out.println("Unalocated Apps: "+f.checkIfAppsAllocated());
		return tot_sec;
	}
	
	public static void Clustering(Fog f){
		print("----- Clustering -----");
		f.clearGwClustConns();
		f.removeClusters();
		Clustering cls = new Clustering(f);
		f.createClusters(cls.DBScan(1,7));//eps, minPts
		
	}
	
	public static void ResourceAllocation(Fog f){
		print("----- Resource Allocation -----");
		f.clearAppToGws();
		f.distributeGw2Cluster();
		
	}

	public static void main(String[] args) {
		//Logger logger = Logger.getRootLogger();
	    SparkConf conf = new SparkConf();
	    JavaSparkContext sc = new JavaSparkContext(conf);
	    String fogFile = "";
	    for (String s :args){
	    	fogFile = s;
	    }
	    createFolder(sc,fogFile);
		Fog f = InitFog(6,0);//Cluster Count, Cloud Gw Count
		writeHDFS(sc,fogFile,"Init",Exporter.writeStringFog(f));	
		//Fog f=Exporter.readJsonFog(readHDFS(sc,fogFile));
	    //readHDFS(sc,fogFile);
		//Global Optimization
		GAGlobalStuff(f, 20, 20);//Fog, Generation Size, Generation Cunt
		writeHDFS(sc,fogFile,"GlobalGA",Exporter.writeStringFog(f));
		// CLustering based GA
		GAClusStuff(f, 20, 20);//Fog, Generation Size, Generation Count
		writeHDFS(sc,fogFile,"ClustGA",Exporter.writeStringFog(f));	    
	    sc.stop();
	  }
	
	//Prints
	public static void print(String str){
		System.out.println(str);
	}
	public static void print(Float str){
		System.out.println(str);
	}
}
